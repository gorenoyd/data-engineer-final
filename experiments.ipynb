{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "45218fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import clickhouse_connect\n",
    "import logging\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, EndpointConnectionError\n",
    "import pandas as pd\n",
    "from io import BytesIO, StringIO\n",
    "import gzip\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, when, upper, trim\n",
    "from pyspark.sql.types import DecimalType\n",
    "import socket\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d95505ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "bronze_storage = {'path': 'http://localhost:9000/', 'user': 'minioadmin', 'pass': 'minioadmin', 'bucket': 'nyc-taxi-raw-data'}\n",
    "clickhouse_storage = {'path': 'http://localhost:9000/', 'user': 'minioadmin', 'pass': 'minioadmin', 'bucket': 'nyc-taxi-db'}\n",
    "\n",
    "clickhouse_host = 'def-clickhouse'\n",
    "clickhouse_port = '8123'\n",
    "clickhouse_user = 'default'\n",
    "clickhouse_password = 'admin'\n",
    "clickhouse_staging_db_name = 'nyc_taxi_staging'\n",
    "clickhouse_silver_db_name = 'nyc_taxi_silver'\n",
    "clickhouse_gold_db_name = 'nyc_taxi_datamarts'\n",
    "\n",
    "raw_data_base_link = 'https://d37ci6vzurychx.cloudfront.net/trip-data/'\n",
    "\n",
    "lookup_table_file_name = 'taxi_zone_lookup.csv'\n",
    "lookup_table_file_url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
    "shape_file_name = 'taxi_zones.zip'\n",
    "shape_file_url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2b9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_db_existance(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Checks if database with db_name exists\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "            FROM system.databases\n",
    "        WHERE name='{db_name}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = bool(ch_client.query(sql).result_rows[0][0])\n",
    "        logging.info(f'Database {db_name} exists: {result}')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error checking database: {e}')\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "166da1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates tabase with db_name\n",
    "    \"\"\"\n",
    "    sql= f\"\"\"\n",
    "        CREATE DATABASE iF NOT EXISTS {db_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'Database {db_name} created successfully')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error creating database: {e}')\n",
    "\n",
    "    return check_db_existance(ch_client, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c3e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_storage(storage_name):\n",
    "    \"\"\"\n",
    "    Checks if storage exists and bucket is accessible with provided credentials and tries to create if it doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=storage_name['path'],\n",
    "            aws_access_key_id=storage_name['user'],\n",
    "            aws_secret_access_key=storage_name['pass']\n",
    "        )\n",
    "\n",
    "        s3.head_bucket(Bucket=storage_name['bucket'])\n",
    "        logging.info(f\"Bucket {storage_name['bucket']} is accessible.\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except EndpointConnectionError as e:\n",
    "        logging.error(f'Cannot connect to endpoint: {e}')\n",
    "    except ClientError as e:\n",
    "        logging.error(f'S3 error: {e}')\n",
    "\n",
    "        # If bucket doesn't exist - try to create\n",
    "        if int(e.response['Error']['Code']) == 404:\n",
    "            logging.error(f\"Bucket {storage_name['bucket']} not found — creating...\")\n",
    "\n",
    "            try:\n",
    "                s3.create_bucket(Bucket=storage_name['bucket'])\n",
    "                print(f\"Bucket {storage_name['bucket']} created.\")\n",
    "            except Exception as ee:\n",
    "                logging.error(f\"Cannot create bucket {storage_name['bucket']}. Error: {ee}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error checking storage: {e}')\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe70493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Bucket nyc-taxi-raw-data is accessible.\n",
      "INFO:root:Bucket nyc-taxi-db is accessible.\n"
     ]
    }
   ],
   "source": [
    "# Check existance of necessary storages\n",
    "for storage in [bronze_storage, clickhouse_storage]:\n",
    "    check_storage(storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c941291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Connected to Clickhouse successfully.\n"
     ]
    }
   ],
   "source": [
    "# Try to connect to ClickHouse instance\n",
    "try:\n",
    "    clickhouse_client = clickhouse_connect.get_client(\n",
    "        host=clickhouse_host,\n",
    "        port=clickhouse_port,\n",
    "        username=clickhouse_user,\n",
    "        password=clickhouse_password)\n",
    "    logging.info(f'Connected to Clickhouse successfully.')\n",
    "except Exception as e:\n",
    "    logging.error(f'Error connecting to Clickhouse: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70aca281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_staging_schema(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates neccessary tables for staging layer, if they don't exist\n",
    "    \"\"\"\n",
    "    logging.info(f'Creating schema for staging layer (database {db_name}), if not exists')\n",
    "    sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {db_name}.bronze_files(\n",
    "            file_name   String NOT NULL,\n",
    "            downloaded_dt  Datetime DEFAULT now(),\n",
    "            source_url  String NOT NULL,\n",
    "            processed   Boolean DEFAULT False,\n",
    "            processed_dt    Datetime DEFAULT NULL\n",
    "        )\n",
    "        ENGINE = ReplacingMergeTree()\n",
    "        ORDER BY file_name\n",
    "        SETTINGS enable_block_number_column = 1,\n",
    "            enable_block_offset_column = 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error creating schema for database {db_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1922b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_schema(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates neccessary tables for silver layer, if they don't exist\n",
    "    \"\"\"\n",
    "    # List of SQL scripts to run\n",
    "    sql_scripts = ['silver.sql', 'populate_vendors.sql', 'populate_rates.sql', 'populate_payment_types.sql']\n",
    "\n",
    "    # Select silver layer database\n",
    "    try:\n",
    "        ch_client.command(f'USE {db_name};')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error selecting database {db_name}: {e}')\n",
    "    \n",
    "    # For each script load it from file and execute\n",
    "    for script in sql_scripts:\n",
    "        with open(script, 'r', encoding='utf-8') as file:\n",
    "            sql = file.read()\n",
    "\n",
    "\n",
    "        # Get individual commands from script\n",
    "        for cmd in sql.split(';'):\n",
    "            cmd = cmd.strip()\n",
    "            if cmd:\n",
    "                try:\n",
    "                    # Execute command\n",
    "                    ch_client.command(cmd)\n",
    "                except Exception as e:\n",
    "                    logging.error(f'Error creating silver layer schema: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf1bd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_schema(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates neccessary tables for gold layer, if they don't exist\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1789e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(ch_client, db_name):\n",
    "    logging.info(f'Checking database {db_name} schema')\n",
    "\n",
    "    if db_name == clickhouse_staging_db_name:\n",
    "        create_staging_schema(ch_client, db_name)\n",
    "    elif db_name == clickhouse_silver_db_name:\n",
    "        create_silver_schema(ch_client, db_name)\n",
    "    elif db_name == clickhouse_gold_db_name:\n",
    "        create_gold_schema(ch_client, db_name)\n",
    "    else:\n",
    "        logging.error(f'Wrong database name!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94090959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Database nyc_taxi_staging exists: True\n",
      "INFO:root:Checking database nyc_taxi_staging schema\n",
      "INFO:root:Creating schema for staging layer (database nyc_taxi_staging), if not exists\n",
      "INFO:root:Database nyc_taxi_silver exists: True\n",
      "INFO:root:Checking database nyc_taxi_silver schema\n",
      "INFO:root:Database nyc_taxi_datamarts exists: True\n",
      "INFO:root:Checking database nyc_taxi_datamarts schema\n"
     ]
    }
   ],
   "source": [
    "# Check existance of necessary databases\n",
    "for db in [clickhouse_staging_db_name, clickhouse_silver_db_name, clickhouse_gold_db_name]:\n",
    "    # If database doesn't exist - create database\n",
    "    if not check_db_existance(clickhouse_client, db):\n",
    "        logging.info(f'Database {db} not found.')\n",
    "        if not create_db(clickhouse_client, db):\n",
    "            logging.error('Cannot create database!')\n",
    "\n",
    "    # Check database schema\n",
    "    check_schema(clickhouse_client, db)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c785796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloaded_files(ch_client, db_name, file_name=''):\n",
    "    \"\"\"\n",
    "    Get list of already downloaded files from database\n",
    "    (may be different from list of files in the bucket)\n",
    "    If file_name parameter is present - returns only corresponding records\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT file_name\n",
    "            FROM {db_name}.bronze_files\n",
    "    \"\"\"\n",
    "    \n",
    "    # If file_name is present - add filter by file name\n",
    "    if file_name:\n",
    "        sql += f\" WHERE file_name = '{file_name}'\"\n",
    "\n",
    "    try:\n",
    "        result = ch_client.query(sql).result_rows\n",
    "        files_list = [r[0] for r in result]\n",
    "\n",
    "        return files_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error getting downloadded files list: {e}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "491ea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_info_to_db(ch_client, db_name, file_name, file_url):\n",
    "    \"\"\"\n",
    "    Writes or updates information on downloaded file into staging database table\n",
    "    Table bronze_files uses ReplacingMergeTree\n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "        INSERT INTO {db_name}.bronze_files (file_name, source_url)\n",
    "            VALUES ('{file_name}', '{file_url}')\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'File {file_name} information saved to database {db_name}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error while saving file {file_name} information to database {db_name}: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0981087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_url, storage):\n",
    "    \"\"\"\n",
    "    Downloads file from given URL and saves to given storage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'Referer': 'https://www.nyc.gov/',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            #'Accept-encoding': 'gzip, deflate, br, zstd',\n",
    "        }\n",
    "        \n",
    "        with requests.get(file_url, headers=headers, stream=True) as file:\n",
    "            # If status is not 2xx - raise error\n",
    "            file.raise_for_status()\n",
    "\n",
    "            # Extract file name from URL\n",
    "            file_name = file_url.split('/')[-1]\n",
    "\n",
    "            try:\n",
    "                s3 = boto3.client(\n",
    "                    's3',\n",
    "                    endpoint_url=storage['path'],\n",
    "                    aws_access_key_id=storage['user'],\n",
    "                    aws_secret_access_key=storage['pass']\n",
    "                )\n",
    "                s3.upload_fileobj(file.raw, storage['bucket'], file_name)\n",
    "                logging.info(f\"File {file_name} uploaded to storage {storage['path']}{storage['bucket']}\")\n",
    "\n",
    "                return True\n",
    "            except Exception as ee:\n",
    "                    logging.error(f\"Error saving file {file_name} to storage {storage['path']}{storage['bucket']}. Error: {ee}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error downloading file {file_url}: {e}')\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd764c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_file_processed(ch_client, db_name, file_name):\n",
    "    \"\"\"\n",
    "    Set field 'processed' in table bronze_files as True for given file\n",
    "    The table uses ReplacingMergeTree engine, so we use INSERT to update\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        ALTER TABLE {db_name}.bronze_files\n",
    "        UPDATE processed = True,\n",
    "            processed_dt = now()\n",
    "        WHERE file_name = '{file_name}';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'File {file_name} marked as processed.')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error marking file {file_name} as processed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f349e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_taxi_zones_into_silver(ch_client, file_name, storage, db_name):\n",
    "    \"\"\"\n",
    "    Insert the data from file with lookuptaxi zones lookup table\n",
    "    into h_taxi_zone hub\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f'Inserting data from file {file_name} into silver layer.')\n",
    "\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=storage['path'],\n",
    "            aws_access_key_id=storage['user'],\n",
    "            aws_secret_access_key=storage['pass']\n",
    "        )\n",
    "\n",
    "        file = s3.get_object(Bucket=storage['bucket'], Key=file_name)\n",
    "\n",
    "        # Make panadas dataframe from csv for further processing\n",
    "        csv_data = file['Body'].read()\n",
    "        df = pd.read_csv(BytesIO(csv_data))\n",
    "\n",
    "        # Drop duplicates\n",
    "        df = df.drop_duplicates(subset=['LocationID'])\n",
    "        # Add 'source' field\n",
    "        df['source'] = 'Data Dictionary – Yellow Taxi Trip Records - March 18, 2025'\n",
    "        # Cast data types just in case\n",
    "        df['LocationID'] = df['LocationID'].astype(int)\n",
    "        df['Borough'] = df['Borough'].astype(str)\n",
    "        df['Zone'] = df['Zone'].astype(str)\n",
    "        df['service_zone'] = df['service_zone'].astype(str)\n",
    "\n",
    "        # Insert into hub\n",
    "        data_to_insert = [tuple(x) for x in df[['LocationID', 'source']].to_numpy()]\n",
    "        ch_client.insert(f'{db_name}.hub_taxi_zone', data_to_insert, column_names=['zone_id', 'record_source'])\n",
    "\n",
    "        # Insert into satellite\n",
    "        data_to_insert = [tuple(x) for x in df[['LocationID', 'Borough', 'Zone', 'service_zone', 'source']].to_numpy()]\n",
    "        ch_client.insert(f'{db_name}.sat_taxi_zone_details', data_to_insert, column_names=['taxi_zone_id', 'borough', 'zone', 'service_zone', 'record_source'])\n",
    "        \n",
    "        # Mark lookup file as processed\n",
    "        mark_file_processed(ch_client, clickhouse_staging_db_name, file_name)\n",
    "        logging.info(f'Taxi zones uploaded into database {db_name}')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error loading taxi zones data into database {db_name}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce3695f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Taxi zones lookup table is already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download lookup table if it is not downloaded already\n",
    "if not len(get_downloaded_files(clickhouse_client, clickhouse_staging_db_name, lookup_table_file_name)):\n",
    "    # Don't use function download_file for this file as it uses streming download\n",
    "    try:\n",
    "        response = requests.get(lookup_table_file_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=bronze_storage['path'],\n",
    "            aws_access_key_id=bronze_storage['user'],\n",
    "            aws_secret_access_key=bronze_storage['pass']\n",
    "        )\n",
    "\n",
    "        s3.upload_fileobj(BytesIO(response.content), bronze_storage['bucket'], lookup_table_file_name)\n",
    "        logging.info(f\"File {lookup_table_file_name} uploaded to storage {bronze_storage['path']}{bronze_storage['bucket']}\")\n",
    "        \n",
    "        write_file_info_to_db(clickhouse_client, clickhouse_staging_db_name, lookup_table_file_name, lookup_table_file_url)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error downloading lookup table {lookup_table_file_url}: {e}')\n",
    "else:\n",
    "    logging.info(f'Taxi zones lookup table is already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1f357ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Inserting data from file taxi_zone_lookup.csv into silver layer.\n",
      "INFO:root:File taxi_zone_lookup.csv marked as processed.\n",
      "INFO:root:Taxi zones uploaded into database nyc_taxi_silver\n"
     ]
    }
   ],
   "source": [
    "# Upload the data into silver layer\n",
    "insert_taxi_zones_into_silver(clickhouse_client, lookup_table_file_name, bronze_storage, clickhouse_silver_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a2ba04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Shape file taxi_zones.zip already exists\n"
     ]
    }
   ],
   "source": [
    "# Download taxi zones shape file if it is not downloaded already\n",
    "try:\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=bronze_storage['path'],\n",
    "        aws_access_key_id=bronze_storage['user'],\n",
    "        aws_secret_access_key=bronze_storage['pass']\n",
    "    )\n",
    "\n",
    "    s3.head_object(Bucket=bronze_storage['bucket'], Key=shape_file_name)\n",
    "    logging.info(f'Shape file {shape_file_name} already exists')\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        logging.error(f\"Shape file {shape_file_name} doesn't exist, downloading\")       \n",
    "        download_file(shape_file_url, bronze_storage)\n",
    "    else:\n",
    "        logging.error(f\"Error checking shape file {shape_file_name} in storage {bronze_storage['path']}{bronze_storage['bucket']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4712896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading file https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet\n",
      "INFO:root:File yellow_tripdata_2009-05.parquet uploaded to storage http://localhost:9000/nyc-taxi-raw-data\n",
      "INFO:root:File yellow_tripdata_2009-05.parquet information saved to database nyc_taxi_staging\n"
     ]
    }
   ],
   "source": [
    "# Download raw data file for each month from the last upload\n",
    "start_date = datetime.strptime('2009-01-01', '%Y-%m-%d')\n",
    "start_year = int(start_date.year)\n",
    "start_month = int(start_date.month)\n",
    "\n",
    "end_date = datetime.now()\n",
    "end_year = int(end_date.year)\n",
    "end_month = int(end_date.month)\n",
    "\n",
    "# Construct a file name for each month and download it\n",
    "tmp_counter = 0\n",
    "\n",
    "# Getting the list of already downloaded files\n",
    "# Checking all files every time the DAG runs allows to download them in any order\n",
    "# and not rely on specific order (as if getting maximum date etc)\n",
    "# Keeping information in the table instead of checking existance in the bucket\n",
    "# allows to drop older processed files wothout them being downloaded again\n",
    "downloaded_files_list = get_downloaded_files(clickhouse_client, clickhouse_staging_db_name)\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for month in range(start_month, 13):\n",
    "        raw_data_file_name = f'yellow_tripdata_{year}-{month:02d}.parquet'\n",
    "        raw_data_url = raw_data_base_link + raw_data_file_name\n",
    "\n",
    "        # If that file has already been downloaded - get next one\n",
    "        if raw_data_file_name in downloaded_files_list:\n",
    "             continue;\n",
    "\n",
    "        logging.info(f'Downloading file {raw_data_url}')\n",
    "        if download_file(raw_data_url, bronze_storage):\n",
    "            write_file_info_to_db(clickhouse_client, clickhouse_staging_db_name, raw_data_file_name, raw_data_url)\n",
    "\n",
    "        tmp_counter += 1\n",
    "        if tmp_counter >= 1:\n",
    "            break\n",
    "\n",
    "    break\n",
    "\n",
    "    # For all years except start_year start_month = 1\n",
    "    start_month = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d6733f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unprocessed_files_list(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Returns list of downloaded, but unprocessed raw data files\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT file_name\n",
    "            FROM {db_name}.bronze_files\n",
    "        WHERE processed == False\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = ch_client.query(sql).result_rows\n",
    "        files_list = [r[0] for r in result]\n",
    "\n",
    "        return files_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error getting unprocessed files list from database {db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82ca54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(ch_client, db_name, table_name):\n",
    "    \"\"\"\n",
    "    Drops table with table_name in database db_name\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {db_name}.{table_name}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error while dropping table {table_name} from database {db_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc691950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bronze_to_staging(ch_client, db_name, file_name, storage):\n",
    "    \"\"\"\n",
    "    Import data from raw data file into staging table\n",
    "    Get all columns of data file and add load_timestamp and record_source fields\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop target table before uploading data\n",
    "    # instead of truncating it\n",
    "    # This approach gives opportunity for schema changes in raw data files\n",
    "    drop_table(ch_client, db_name, 'staging_data')\n",
    "\n",
    "    storage = {'path': 'http://def-minio:9000/', 'user': 'minioadmin', 'pass': 'minioadmin', 'bucket': 'nyc-taxi-raw-data'}\n",
    "\n",
    "    # Create table from raw data file\n",
    "    sql = f\"\"\"\n",
    "        CREATE TABLE {db_name}.staging_data\n",
    "        ENGINE = Log\n",
    "        AS\n",
    "        SELECT DISTINCT *, now() AS load_timestamp, \\'{storage['path']}{storage['bucket']}/{file_name}\\' AS record_source\n",
    "            FROM s3(\n",
    "                \\'{storage['path']}{storage['bucket']}/{file_name}\\',\n",
    "                'minioadmin',\n",
    "                'minioadmin',\n",
    "                'Parquet'\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'Data from file {file_name} successfully uploaded to database {db_name}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error while uploading data from file {file_name} into database {db_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd88540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone_ids(df, spark):\n",
    "    \"\"\"\n",
    "    Find zone id in shape file by coordinates\n",
    "    \"\"\"\n",
    "    logging.info(f'Finding zone ids in shape file')\n",
    "\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=bronze_storage['path'],\n",
    "            aws_access_key_id=bronze_storage['user'],\n",
    "            aws_secret_access_key=bronze_storage['pass']\n",
    "        )\n",
    "\n",
    "        # Get zip file from storage\n",
    "        shp_zip_file_name = shape_file_url.split('/')[-1]\n",
    "        print(shp_zip_file_name)\n",
    "        obj = s3.get_object(Bucket=bronze_storage['bucket'], Key=shp_zip_file_name)\n",
    "        zip_data = BytesIO(obj['Body'].read())\n",
    "\n",
    "        # Read shapes from zip\n",
    "        zones = gpd.read_file(f'zip://{zip_data}')\n",
    "        # Convert CRS\n",
    "        zones = zones.to_crs(epsg=4326)\n",
    "        # Read shape file\n",
    "        print(zones)\n",
    "        return\n",
    "        zones = spark.read.format('geospark') \\\n",
    "            .option('geomField', 'geometry') \\\n",
    "            .load(f\"s3a:///{bronze_storage['bucket']}/{shape_file_name}\")\n",
    "        print('c')\n",
    "        # Transform coordinates in dataframe into geometry\n",
    "        df = df.withColumn('pickup_geom', \\\n",
    "            expr('ST_Point(cast(start_lon as Decimal(9,6)), cast(start_lat as Decimal(9,6)))')) \\\n",
    "            .withColumn('dropoff_geom', \\\n",
    "            expr('ST_Point(cast(end_lon as Decimal(9,6)), cast(end_lat as Decimal(9,6)))'))\n",
    "        print('d')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading shape file {shape_file_name} in storage {bronze_storage['path']}{bronze_storage['bucket']}: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f17ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Connection to database nyc_taxi_staging successful. Processing data for silver layer.\n",
      "a\n",
      "INFO:root:Finding zone ids in shape file\n",
      "ERROR:root:Error loading shape file taxi_zones.shp in storage http://localhost:9000/nyc-taxi-raw-data: '/vsizip/<_io.BytesIO object at 0x0000023B19A84950>' does not exist in the file system, and is not recognized as a supported dataset name.\n",
      "b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_to_silver(clickhouse_staging_db_name, clickhouse_silver_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c171334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outbursts(df, column_name):\n",
    "    \"\"\"\n",
    "    Drop rows outside 1.5 IQR\n",
    "    \"\"\"\n",
    "    # Calculate quantiles\n",
    "    q1, q3 = df.approxQuantile(column_name, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Calculate boundaries\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    # Drop rows outside boudaries\n",
    "    df_clean = df.filter((F.col(column_name) >= lower) & (F.col(column_name) <= upper))\n",
    "    \n",
    "    logging.info(f'Deleted outbursts for column {column_name}')\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46852ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_to_silver(staging_db, silver_db):\n",
    "    \"\"\"\n",
    "    Process data from staging table and insert into silver layer\n",
    "    \"\"\"\n",
    "    # Process staging data using Spark\n",
    "    try:\n",
    "        spark.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('Staging2Silver') \\\n",
    "        .master('spark://spark-master:7077') \\\n",
    "        .config('spark.driver.host', 'host.docker.internal') \\\n",
    "        .config('spark.driver.bindAddress', '0.0.0.0') \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "            \"org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.6.1,\"\n",
    "            \"org.datasyslab:geotools-wrapper:geotools-24.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .format('jdbc') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{clickhouse_staging_db_name}') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('dbtable', 'staging_data') \\\n",
    "            .option('user', f'{clickhouse_user}') \\\n",
    "            .option('password', f'{clickhouse_password}') \\\n",
    "            .load()\n",
    "        logging.info(f'Connection to database {staging_db} successful. Processing data for silver layer.')\n",
    "        #df.show(5)\n",
    "        print('a')\n",
    "        get_zone_ids(df, spark)\n",
    "        print('b')\n",
    "        return 0\n",
    "        logging.info(f'Rows before cleaning: {df.count()}')\n",
    "\n",
    "        # By default suppose that dataframe doesn't contain taxi zone ids\n",
    "        df_has_location_ids = False\n",
    "\n",
    "        # Check columns one by one\n",
    "        # Schema and semantics of data have changed over time\n",
    "        for column in df.columns:\n",
    "            column_name = column.lower()\n",
    "\n",
    "            # Check if column contains LocationID - can be PULocationID and DOLocationID\n",
    "            if 'locationid' in column_name:\n",
    "                df_has_location_ids = True\n",
    "                # Drop rows with null and negative values and values greater than 500\n",
    "                df = df.filter(col(column)).isNotNull & (col(column) > 0) & (col(column) <= 500)\n",
    "\n",
    "            # In early years 'vendor' column was named vendor_name and contained string codes\n",
    "            if column_name == 'vendor_name':\n",
    "                # Replace vendor string code to index and covert to int type\n",
    "                df = df.withColumn(column, \\\n",
    "                    when(upper(trim(col(column))) == 'CMT', 1) \\\n",
    "                    .when(upper(trim(col(column))) == 'VTS', 101) \\\n",
    "                    .when(upper(trim(col(column))) == 'DDS', 102) \\\n",
    "                    .otherwise(999) \\\n",
    "                    .cast('int'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'vendor_id')\n",
    "            \n",
    "            # Later 'vendor' column was named vendorid and contained integer ids\n",
    "            elif column_name == 'vendorid':\n",
    "                # Convert columnt to int\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast('int'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'vendor_id')\n",
    "            \n",
    "            # Pickup datetime\n",
    "            elif 'pickup_datetime' in column_name:\n",
    "                # Drop rows with null\n",
    "                df = df.filter(col(column).isNotNull())\n",
    "                # Convert to datetime\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast('timestamp'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'pickup_datetime')\n",
    "\n",
    "            # Dropoff datetime\n",
    "            elif 'dropoff_datetime' in column_name:\n",
    "                # Drop rows with null\n",
    "                df = df.filter(col(column).isNotNull())\n",
    "                # Convert to datetime\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast('timestamp'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'dropoff_datetime')\n",
    "\n",
    "            # Passenger count - can be 0 or null\n",
    "            elif 'passenger' in column_name:\n",
    "                # Drop negative values and values greater than 10\n",
    "                df = df.filter(col(column).isNull() | ((col(column).cast('int') >= 0) & (col(column).cast('int') <= 10)))\n",
    "                # Convert to int\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast('int'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'passenger_count')\n",
    "\n",
    "            # Trip distance - can be 0 or null\n",
    "            elif 'distance' in column_name:\n",
    "                # Drop negative values and values greater than 1000 miles\n",
    "                df = df.filter(col(column).isNull() | ((col(column).cast('int') >= 0) & (col(column).cast('int') <= 1000)))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(6, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'trip_distance')\n",
    "\n",
    "            # Rate id\n",
    "            elif 'rate' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to int\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast('int'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'rate_id')\n",
    "\n",
    "            # Store and forward flag\n",
    "            elif 'store' in column_name:\n",
    "                # Convert to bool\n",
    "                df = df.withColumn(column, \\\n",
    "                    when(upper(trim(col(column))) == 'Y', 1) \\\n",
    "                    .when(upper(trim(col(column))) == 'N', 0) \\\n",
    "                    .otherwise(None)\n",
    "                    .cast('boolean'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'store_and_fwd')\n",
    "\n",
    "            # Payment type - can be string or int id\n",
    "            elif 'payment' in column_name:\n",
    "                # Replace strings for indexes\n",
    "                df = df.withColumn(column, \\\n",
    "                    when(upper(trim(col(column))) == 'CREDIT', 1) \\\n",
    "                    .when(upper(trim(col(column))) == 'CRD', 1) \\\n",
    "                    .when(upper(trim(col(column))) == 'CASH', 2) \\\n",
    "                    .when(upper(trim(col(column))) == 'CSH', 2) \\\n",
    "                    .when(upper(trim(col(column))) == 'NO CHARGE', 3) \\\n",
    "                    .when(upper(trim(col(column))) == 'NOC', 3) \\\n",
    "                    .when(upper(trim(col(column))) == 'DISPUTE', 4) \\\n",
    "                    .when(upper(trim(col(column))) == 'DIS', 4) \\\n",
    "                    .when(upper(trim(col(column))) == 'UNK', 5) \\\n",
    "                    .when(col(column).isNull(), 5) \\\n",
    "                    .cast('int'))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'payment_type_id')\n",
    "\n",
    "            # Fare amount\n",
    "            elif 'fare_am' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(6, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'fare_amount')\n",
    "\n",
    "            # Surcharges\n",
    "            elif 'extra' in column_name or 'surcharge' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(6, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'extra')\n",
    "\n",
    "            # MTA tax\n",
    "            elif 'mta_tax' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(5, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'mta_tax')\n",
    "\n",
    "            # Tip amount (only for credit cards)\n",
    "            elif 'tip_am' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(6, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'tip_amount')\n",
    "\n",
    "            # Tolls amount\n",
    "            elif 'tolls_am' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(6, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'tolls_amount')\n",
    "\n",
    "            # Improvement surcharge\n",
    "            elif 'improvement' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(5, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'improvement_surcharge')\n",
    "\n",
    "            # Total amount\n",
    "            elif 'total_am' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(6, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'total_amount')\n",
    "\n",
    "            # Congestion surcharge\n",
    "            elif column_name == 'congestion_surcharge' in column_name:\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(5, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'congestion_surcharge')\n",
    "\n",
    "            # Airport fee\n",
    "            elif column_name == 'airport_fee':\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(5, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'airport_fee')\n",
    "\n",
    "            # CBD congestion surcharge\n",
    "            elif column_name == 'cbd_congestion_fee':\n",
    "                # Drop negative values\n",
    "                df = df.filter(col(column).isNull() | (col(column).cast('int') >= 0))\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(5, 2)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, 'cbd_congestion_fee')\n",
    "\n",
    "            # Pickup and dropoff location coordinates\n",
    "            elif 'start_' in column_name or 'end_' in column_name:\n",
    "                # Drop null values\n",
    "                df = df.filter(col(column).isNotNull())\n",
    "                # Drop outbursts\n",
    "                df = drop_outbursts(df, column)\n",
    "\n",
    "                # Convert to decimal\n",
    "                df = df.withColumn(column, \\\n",
    "                    col(column).cast(DecimalType(9, 6)))\n",
    "                # Rename column\n",
    "                df = df.withColumnRenamed(column, column_name)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        df.show(5)\n",
    "        logging.info(f'Rows after cleaning: {df.count()}')\n",
    "\n",
    "        # Find taxi zone number by coordinates\n",
    "        if not df_has_location_ids:\n",
    "            df = get_zone_ids(df, spark)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error connecting to database {staging_db} from Spark cluster: {e}')\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            spark.stop()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ba0993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark master доступен!\n"
     ]
    }
   ],
   "source": [
    "def is_spark_master_available(host='localhost', port=7077, timeout=3):\n",
    "    try:\n",
    "        with socket.create_connection((host, port), timeout=timeout):\n",
    "            return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "if is_spark_master_available():\n",
    "    print('Spark master доступен!')\n",
    "else:\n",
    "    print('Не удалось подключиться к Spark master.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "990a5b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Connection to database nyc_taxi_staging successful. Processing data for silver layer.\n",
      "a\n",
      "ERROR:root:Error connecting to database nyc_taxi_staging from Spark cluster: 'JavaPackage' object is not callable\n"
     ]
    }
   ],
   "source": [
    "#spark.stop()\n",
    "staging_to_silver(clickhouse_staging_db_name, clickhouse_silver_db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffa7979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_to_gold(file_name):\n",
    "    \"\"\"\n",
    "    Create datamarts from silver layer\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\t\\t', file_name, 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80de26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Data from file yellow_tripdata_2009-02.parquet successfully uploaded to database nyc_taxi_staging\n"
     ]
    }
   ],
   "source": [
    "# For each downloaded and unprocessed file with raw data\n",
    "# - import to staging\n",
    "# - process to silver\n",
    "# - process to gold\n",
    "unprocessed_files_list = get_unprocessed_files_list(clickhouse_client, clickhouse_staging_db_name)\n",
    "\n",
    "for file in unprocessed_files_list:\n",
    "    bronze_to_staging(clickhouse_client, clickhouse_staging_db_name, file, bronze_storage)\n",
    "    #staging_to_silver(clickhouse_staging_db_name, clickhouse_silver_db_name)\n",
    "    #silver_to_gold(file)\n",
    "\n",
    "    # На время отладки\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204c451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf75146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем данные в силвер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем 27 и 28 в цикле до текущего месяца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3968b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
