{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45218fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import clickhouse_connect\n",
    "import logging\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, EndpointConnectionError\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, when, upper, trim, concat_ws, sha2\n",
    "from pyspark.sql.types import FloatType\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95505ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "bronze_storage = {'path': 'http://localhost:9000/', 'user': 'minioadmin', 'pass': 'minioadmin', 'bucket': 'nyc-taxi-raw-data'}\n",
    "clickhouse_storage = {'path': 'http://localhost:9000/', 'user': 'minioadmin', 'pass': 'minioadmin', 'bucket': 'nyc-taxi-db'}\n",
    "\n",
    "clickhouse_host = 'def-clickhouse'\n",
    "clickhouse_port = '8123'\n",
    "clickhouse_user = 'default'\n",
    "clickhouse_password = 'admin'\n",
    "clickhouse_staging_db_name = 'nyc_taxi_staging'\n",
    "clickhouse_silver_db_name = 'nyc_taxi_silver'\n",
    "clickhouse_gold_db_name = 'nyc_taxi_datamarts'\n",
    "\n",
    "raw_data_base_link = 'https://d37ci6vzurychx.cloudfront.net/trip-data/'\n",
    "\n",
    "lookup_table_file_name = 'taxi_zone_lookup.csv'\n",
    "lookup_table_file_url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
    "\n",
    "spark_master = 'spark://spark-master:7077'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2b9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_db_existance(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Checks if database with db_name exists\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "            FROM system.databases\n",
    "        WHERE name='{db_name}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = bool(ch_client.query(sql).result_rows[0][0])\n",
    "        logging.info(f'Database {db_name} exists: {result}')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error checking database: {e}')\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166da1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates tabase with db_name\n",
    "    \"\"\"\n",
    "    sql= f\"\"\"\n",
    "        CREATE DATABASE iF NOT EXISTS {db_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'Database {db_name} created successfully')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error creating database: {e}')\n",
    "\n",
    "    return check_db_existance(ch_client, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c3e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_storage(storage_name):\n",
    "    \"\"\"\n",
    "    Checks if storage exists and bucket is accessible with provided credentials and tries to create if it doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=storage_name['path'],\n",
    "            aws_access_key_id=storage_name['user'],\n",
    "            aws_secret_access_key=storage_name['pass']\n",
    "        )\n",
    "\n",
    "        s3.head_bucket(Bucket=storage_name['bucket'])\n",
    "        logging.info(f\"Bucket {storage_name['bucket']} is accessible.\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except EndpointConnectionError as e:\n",
    "        logging.error(f'Cannot connect to endpoint: {e}')\n",
    "    except ClientError as e:\n",
    "        logging.error(f'S3 error: {e}')\n",
    "\n",
    "        # If bucket doesn't exist - try to create\n",
    "        if int(e.response['Error']['Code']) == 404:\n",
    "            logging.error(f\"Bucket {storage_name['bucket']} not found — creating...\")\n",
    "\n",
    "            try:\n",
    "                s3.create_bucket(Bucket=storage_name['bucket'])\n",
    "                print(f\"Bucket {storage_name['bucket']} created.\")\n",
    "            except Exception as ee:\n",
    "                logging.error(f\"Cannot create bucket {storage_name['bucket']}. Error: {ee}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error checking storage: {e}')\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe70493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Bucket nyc-taxi-raw-data is accessible.\n",
      "INFO:root:Bucket nyc-taxi-db is accessible.\n"
     ]
    }
   ],
   "source": [
    "# Check existance of necessary storages\n",
    "for storage in [bronze_storage, clickhouse_storage]:\n",
    "    check_storage(storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c941291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Connected to Clickhouse successfully.\n"
     ]
    }
   ],
   "source": [
    "# Try to connect to ClickHouse instance\n",
    "try:\n",
    "    clickhouse_client = clickhouse_connect.get_client(\n",
    "        host=clickhouse_host,\n",
    "        port=clickhouse_port,\n",
    "        username=clickhouse_user,\n",
    "        password=clickhouse_password)\n",
    "    logging.info(f'Connected to Clickhouse successfully.')\n",
    "except Exception as e:\n",
    "    logging.error(f'Error connecting to Clickhouse: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70aca281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_staging_schema(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates neccessary tables for staging layer, if they don't exist\n",
    "    \"\"\"\n",
    "    logging.info(f'Creating schema for staging layer (database {db_name}), if not exists')\n",
    "    sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {db_name}.bronze_files(\n",
    "            file_name   String NOT NULL,\n",
    "            downloaded_dt  Datetime DEFAULT now(),\n",
    "            source_url  String NOT NULL,\n",
    "            processed   Boolean DEFAULT False,\n",
    "            processed_dt    Datetime DEFAULT NULL\n",
    "        )\n",
    "        ENGINE = ReplacingMergeTree()\n",
    "        ORDER BY file_name\n",
    "        SETTINGS enable_block_number_column = 1,\n",
    "            enable_block_offset_column = 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error creating schema for database {db_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1922b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_schema(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates neccessary tables for silver layer, if they don't exist\n",
    "    \"\"\"\n",
    "    # List of SQL scripts to run\n",
    "    sql_scripts = ['silver.sql', 'populate_vendors.sql', 'populate_rates.sql', 'populate_payment_types.sql']\n",
    "\n",
    "    # Select silver layer database\n",
    "    try:\n",
    "        ch_client.command(f'USE {db_name};')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error selecting database {db_name}: {e}')\n",
    "    \n",
    "    # For each script load it from file and execute\n",
    "    for script in sql_scripts:\n",
    "        with open(script, 'r', encoding='utf-8') as file:\n",
    "            sql = file.read()\n",
    "\n",
    "\n",
    "        # Get individual commands from script\n",
    "        for cmd in sql.split(';'):\n",
    "            cmd = cmd.strip()\n",
    "            if cmd:\n",
    "                try:\n",
    "                    # Execute command\n",
    "                    ch_client.command(cmd)\n",
    "                except Exception as e:\n",
    "                    logging.error(f'Error creating silver layer schema: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf1bd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_schema(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Creates neccessary tables for gold layer, if they don't exist\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1789e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(ch_client, db_name):\n",
    "    logging.info(f'Checking database {db_name} schema')\n",
    "\n",
    "    if db_name == clickhouse_staging_db_name:\n",
    "        create_staging_schema(ch_client, db_name)\n",
    "    elif db_name == clickhouse_silver_db_name:\n",
    "        create_silver_schema(ch_client, db_name)\n",
    "    elif db_name == clickhouse_gold_db_name:\n",
    "        create_gold_schema(ch_client, db_name)\n",
    "    else:\n",
    "        logging.error(f'Wrong database name!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94090959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Database nyc_taxi_staging exists: True\n",
      "INFO:root:Checking database nyc_taxi_staging schema\n",
      "INFO:root:Creating schema for staging layer (database nyc_taxi_staging), if not exists\n",
      "INFO:root:Database nyc_taxi_silver exists: True\n",
      "INFO:root:Checking database nyc_taxi_silver schema\n",
      "INFO:root:Database nyc_taxi_datamarts exists: True\n",
      "INFO:root:Checking database nyc_taxi_datamarts schema\n"
     ]
    }
   ],
   "source": [
    "# Check existance of necessary databases\n",
    "for db in [clickhouse_staging_db_name, clickhouse_silver_db_name, clickhouse_gold_db_name]:\n",
    "    # If database doesn't exist - create database\n",
    "    if not check_db_existance(clickhouse_client, db):\n",
    "        logging.info(f'Database {db} not found.')\n",
    "        if not create_db(clickhouse_client, db):\n",
    "            logging.error('Cannot create database!')\n",
    "\n",
    "    # Check database schema\n",
    "    check_schema(clickhouse_client, db)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c785796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloaded_files(ch_client, db_name, file_name=''):\n",
    "    \"\"\"\n",
    "    Get list of already downloaded files from database\n",
    "    (may be different from list of files in the bucket)\n",
    "    If file_name parameter is present - returns only corresponding records\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT file_name\n",
    "            FROM {db_name}.bronze_files\n",
    "    \"\"\"\n",
    "    \n",
    "    # If file_name is present - add filter by file name\n",
    "    if file_name:\n",
    "        sql += f\" WHERE file_name = '{file_name}'\"\n",
    "\n",
    "    try:\n",
    "        result = ch_client.query(sql).result_rows\n",
    "        files_list = [r[0] for r in result]\n",
    "\n",
    "        return files_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error getting downloadded files list: {e}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "491ea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_info_to_db(ch_client, db_name, file_name, file_url):\n",
    "    \"\"\"\n",
    "    Writes or updates information on downloaded file into staging database table\n",
    "    Table bronze_files uses ReplacingMergeTree\n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "        INSERT INTO {db_name}.bronze_files (file_name, source_url)\n",
    "            VALUES ('{file_name}', '{file_url}')\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'File {file_name} information saved to database {db_name}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error while saving file {file_name} information to database {db_name}: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0981087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_url, storage):\n",
    "    \"\"\"\n",
    "    Downloads file from given URL and saves to given storage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'Referer': 'https://www.nyc.gov/',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            #'Accept-encoding': 'gzip, deflate, br, zstd',\n",
    "        }\n",
    "        \n",
    "        with requests.get(file_url, headers=headers, stream=True) as file:\n",
    "            # If status is not 2xx - raise error\n",
    "            file.raise_for_status()\n",
    "\n",
    "            # Extract file name from URL\n",
    "            file_name = file_url.split('/')[-1]\n",
    "\n",
    "            try:\n",
    "                s3 = boto3.client(\n",
    "                    's3',\n",
    "                    endpoint_url=storage['path'],\n",
    "                    aws_access_key_id=storage['user'],\n",
    "                    aws_secret_access_key=storage['pass']\n",
    "                )\n",
    "                s3.upload_fileobj(file.raw, storage['bucket'], file_name)\n",
    "                logging.info(f\"File {file_name} uploaded to storage {storage['path']}{storage['bucket']}\")\n",
    "\n",
    "                return True\n",
    "            except Exception as ee:\n",
    "                    logging.error(f\"Error saving file {file_name} to storage {storage['path']}{storage['bucket']}. Error: {ee}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error downloading file {file_url}: {e}')\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd764c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_file_processed(ch_client, db_name, file_name):\n",
    "    \"\"\"\n",
    "    Set field 'processed' in table bronze_files as True for given file\n",
    "    The table uses ReplacingMergeTree engine, so we use INSERT to update\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        ALTER TABLE {db_name}.bronze_files\n",
    "        UPDATE processed = True,\n",
    "            processed_dt = now()\n",
    "        WHERE file_name = '{file_name}';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'File {file_name} marked as processed.')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error marking file {file_name} as processed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f349e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_taxi_zones_into_silver(ch_client, file_name, storage, db_name):\n",
    "    \"\"\"\n",
    "    Insert the data from file with lookuptaxi zones lookup table\n",
    "    into h_taxi_zone hub\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f'Inserting data from file {file_name} into silver layer.')\n",
    "\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=storage['path'],\n",
    "            aws_access_key_id=storage['user'],\n",
    "            aws_secret_access_key=storage['pass']\n",
    "        )\n",
    "\n",
    "        file = s3.get_object(Bucket=storage['bucket'], Key=file_name)\n",
    "\n",
    "        # Make panadas dataframe from csv for further processing\n",
    "        csv_data = file['Body'].read()\n",
    "        df = pd.read_csv(BytesIO(csv_data))\n",
    "\n",
    "        # Drop duplicates\n",
    "        df = df.drop_duplicates(subset=['LocationID'])\n",
    "        # Add 'source' field\n",
    "        df['source'] = 'Data Dictionary – Yellow Taxi Trip Records - March 18, 2025'\n",
    "        # Cast data types just in case\n",
    "        df['LocationID'] = df['LocationID'].astype(int)\n",
    "        df['Borough'] = df['Borough'].astype(str)\n",
    "        df['Zone'] = df['Zone'].astype(str)\n",
    "        df['service_zone'] = df['service_zone'].astype(str)\n",
    "\n",
    "        # Check existig IDs\n",
    "        sql = f'SELECT zone_id FROM {db_name}.hub_taxi_zone'\n",
    "        result = ch_client.query(sql)\n",
    "        existing_ids = set(row[0] for row in result.result_rows)\n",
    "\n",
    "        # In new dataframe leave only unique rows\n",
    "        new_rows = df[~df['LocationID'].isin(existing_ids)]\n",
    "\n",
    "        # If new rows are present - write them into tables\n",
    "        if not new_rows.empty:\n",
    "            # Insert new rows into hub\n",
    "            data_to_insert = [tuple(x) for x in new_rows[['LocationID', 'source']].to_numpy()]\n",
    "            ch_client.insert(f'{db_name}.hub_taxi_zone', data_to_insert, column_names=['zone_id',   'record_source'])\n",
    "\n",
    "            # Insert new rows into satellite\n",
    "            data_to_insert = [tuple(x) for x in new_rows[['LocationID', 'Borough', 'Zone', 'service_zone', 'source']].to_numpy()]\n",
    "            ch_client.insert(f'{db_name}.sat_taxi_zone_details', data_to_insert, column_names=['taxi_zone_id', 'borough', 'zone', 'service_zone', 'record_source'])\n",
    "\n",
    "            # Mark lookup file as processed\n",
    "            mark_file_processed(ch_client, clickhouse_staging_db_name, file_name)\n",
    "            logging.info(f'Taxi zones uploaded into database {db_name}')\n",
    "        else:\n",
    "            logging.info(f'There are no new records in lookup table')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error loading taxi zones data into database {db_name}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3695f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Taxi zones lookup table is already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download lookup table if it is not downloaded already\n",
    "if not len(get_downloaded_files(clickhouse_client, clickhouse_staging_db_name, lookup_table_file_name)):\n",
    "    # Don't use function download_file for this file as it uses streming download\n",
    "    try:\n",
    "        response = requests.get(lookup_table_file_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=bronze_storage['path'],\n",
    "            aws_access_key_id=bronze_storage['user'],\n",
    "            aws_secret_access_key=bronze_storage['pass']\n",
    "        )\n",
    "\n",
    "        s3.upload_fileobj(BytesIO(response.content), bronze_storage['bucket'], lookup_table_file_name)\n",
    "        logging.info(f\"File {lookup_table_file_name} uploaded to storage {bronze_storage['path']}{bronze_storage['bucket']}\")\n",
    "        \n",
    "        write_file_info_to_db(clickhouse_client, clickhouse_staging_db_name, lookup_table_file_name, lookup_table_file_url)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error downloading lookup table {lookup_table_file_url}: {e}')\n",
    "else:\n",
    "    logging.info(f'Taxi zones lookup table is already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1f357ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Inserting data from file taxi_zone_lookup.csv into silver layer.\n",
      "INFO:root:There are no new records in lookup table\n"
     ]
    }
   ],
   "source": [
    "# Upload the data into silver layer\n",
    "insert_taxi_zones_into_silver(clickhouse_client, lookup_table_file_name, bronze_storage, clickhouse_silver_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4712896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading file https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-01.parquet\n",
      "INFO:root:File yellow_tripdata_2017-01.parquet uploaded to storage http://localhost:9000/nyc-taxi-raw-data\n",
      "INFO:root:File yellow_tripdata_2017-01.parquet information saved to database nyc_taxi_staging\n"
     ]
    }
   ],
   "source": [
    "# Download raw data file for each month from the last upload\n",
    "start_date = datetime.strptime('2017-01-01', '%Y-%m-%d')\n",
    "start_year = int(start_date.year)\n",
    "start_month = int(start_date.month)\n",
    "\n",
    "end_date = datetime.now()\n",
    "end_year = int(end_date.year)\n",
    "end_month = int(end_date.month)\n",
    "\n",
    "# Construct a file name for each month and download it\n",
    "tmp_counter = 0\n",
    "\n",
    "# Getting the list of already downloaded files\n",
    "# Checking all files every time the DAG runs allows to download them in any order\n",
    "# and not rely on specific order (as if getting maximum date etc)\n",
    "# Keeping information in the table instead of checking existance in the bucket\n",
    "# allows to drop older processed files wothout them being downloaded again\n",
    "downloaded_files_list = get_downloaded_files(clickhouse_client, clickhouse_staging_db_name)\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for month in range(start_month, 13):\n",
    "        raw_data_file_name = f'yellow_tripdata_{year}-{month:02d}.parquet'\n",
    "        raw_data_url = raw_data_base_link + raw_data_file_name\n",
    "\n",
    "        # If that file has already been downloaded - get next one\n",
    "        if raw_data_file_name in downloaded_files_list:\n",
    "             continue;\n",
    "\n",
    "        logging.info(f'Downloading file {raw_data_url}')\n",
    "        if download_file(raw_data_url, bronze_storage):\n",
    "            write_file_info_to_db(clickhouse_client, clickhouse_staging_db_name, raw_data_file_name, raw_data_url)\n",
    "\n",
    "        tmp_counter += 1\n",
    "        if tmp_counter >= 1:\n",
    "            break\n",
    "\n",
    "    break\n",
    "\n",
    "    # For all years except start_year start_month = 1\n",
    "    start_month = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d6733f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unprocessed_files_list(ch_client, db_name):\n",
    "    \"\"\"\n",
    "    Returns list of downloaded, but unprocessed raw data files\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT file_name\n",
    "            FROM {db_name}.bronze_files\n",
    "        WHERE processed == False\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = ch_client.query(sql).result_rows\n",
    "        files_list = [r[0] for r in result]\n",
    "\n",
    "        return files_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error getting unprocessed files list from database {db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82ca54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(ch_client, db_name, table_name):\n",
    "    \"\"\"\n",
    "    Drops table with table_name in database db_name\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {db_name}.{table_name}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error while dropping table {table_name} from database {db_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc691950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bronze_to_staging(ch_client, db_name, file_name, storage):\n",
    "    \"\"\"\n",
    "    Import data from raw data file into staging table\n",
    "    Get all columns of data file and add load_timestamp and record_source fields\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop target table before uploading data\n",
    "    # instead of truncating it\n",
    "    # This approach gives opportunity for schema changes in raw data files\n",
    "    drop_table(ch_client, db_name, 'staging_data')\n",
    "\n",
    "    storage = {'path': 'http://def-minio:9000/', 'user': 'minioadmin', 'pass': 'minioadmin', 'bucket': 'nyc-taxi-raw-data'}\n",
    "\n",
    "    # Set option to ignore columns with only nulls\n",
    "    sql = 'SET input_format_parquet_skip_columns_with_unsupported_types_in_schema_inference = 1;'\n",
    "    ch_client.command(sql)\n",
    "\n",
    "    # Create table from raw data file\n",
    "    sql = f\"\"\"\n",
    "        CREATE TABLE {db_name}.staging_data\n",
    "        ENGINE = Log\n",
    "        AS\n",
    "        SELECT DISTINCT *, now() AS load_timestamp, \\'{storage['path']}{storage['bucket']}/{file_name}\\' AS record_source\n",
    "            FROM s3(\n",
    "                \\'{storage['path']}{storage['bucket']}/{file_name}\\',\n",
    "                'minioadmin',\n",
    "                'minioadmin',\n",
    "                'Parquet'\n",
    "            )\n",
    "        LIMIT 100000\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ch_client.command(sql)\n",
    "        logging.info(f'Data from file {file_name} successfully uploaded to database {db_name}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error while uploading data from file {file_name} into database {db_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c171334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df, column_name):\n",
    "    \"\"\"\n",
    "    Drop values outside 1.5 IQR\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate quantiles\n",
    "    q1, q3 = df.approxQuantile(column_name, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Calculate boundaries\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    # Drop rows outside boudaries\n",
    "    df_clean = df.filter((F.col(column_name) >= lower) & (F.col(column_name) <= upper))\n",
    "    \n",
    "    logging.info(f'Deleted outliers for column {column_name}')\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8f6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_staging_data(df):\n",
    "    \"\"\"\n",
    "    Cleans data from staging table\n",
    "    \"\"\"\n",
    "    # List of money amount columns - all can be 0 or null\n",
    "    money_columns = ['fare_amount',\n",
    "                    'extra',\n",
    "                    'mta_tax',\n",
    "                    'tip_amount',\n",
    "                    'tolls_amount',\n",
    "                    'improvement_surcharge',\n",
    "                    'total_amount',\n",
    "                    'congestion_surcharge',\n",
    "                    'airport_fee',\n",
    "                    'cbd_congestion_fee']\n",
    "\n",
    "    logging.info(f'Rows before cleaning: {df.count()}')\n",
    "\n",
    "    # Drop duplicates by start date and location, end date and location and price\n",
    "    df = df.dropDuplicates(['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'total_amount'])\n",
    "\n",
    "    # Check columns one by one\n",
    "    for column in df.columns:\n",
    "        # Information vendor\n",
    "        if column == 'VendorID':\n",
    "            # Convert columnt to int\n",
    "            df = df.withColumn(column, col(column).cast('int'))\n",
    "            # Drop negative and too big values\n",
    "            df = df.filter(col(column).isNull() | ((col(column) > 0) & (col(column) <= 1000)))\n",
    "        \n",
    "        # Pickup datetime\n",
    "        elif column == 'tpep_pickup_datetime':\n",
    "            # Drop rows with null values\n",
    "            df = df.filter(col(column).isNotNull())\n",
    "            # Convert to datetime\n",
    "            df = df.withColumn(column, col(column).cast('timestamp'))\n",
    "\n",
    "        # Dropoff datetime\n",
    "        elif column == 'tpep_dropoff_datetime':\n",
    "            # Drop rows with null\n",
    "            df = df.filter(col(column).isNotNull())\n",
    "            # Convert to datetime\n",
    "            df = df.withColumn(column, col(column).cast('timestamp'))\n",
    "\n",
    "        # Passenger count - can be 0 or null for voided trips etc\n",
    "        elif column == 'passenger_count':\n",
    "            # Convert to int\n",
    "            df = df.withColumn(column, col(column).cast('int'))\n",
    "            # Drop negative values and values greater than 10\n",
    "            df = df.filter(col(column).isNull() | ((col(column) >= 0) & (col(column) <= 10)))\n",
    "\n",
    "\n",
    "        # Trip distance - can be 0 or null for voided trips etc\n",
    "        elif column == 'trip_distance':\n",
    "            # Convert to float\n",
    "            df = df.withColumn(column, col(column).cast(FloatType()))\n",
    "            # Drop negative values and values greater than 1000 miles\n",
    "            df = df.filter(col(column).isNull() | ((col(column) >= 0) & (col(column) <= 1000)))\n",
    "            # Drop outliers\n",
    "            df = drop_outliers(df, column)\n",
    "\n",
    "        # Rate id\n",
    "        elif column == 'RatecodeID':\n",
    "            # Convert to int\n",
    "            df = df.withColumn(column, col(column).cast('int'))\n",
    "            # Drop negative and too big values\n",
    "            df = df.filter(col(column).isNull() | ((col(column) > 0) & (col(column) <= 1000)))\n",
    "\n",
    "        # Store and forward flag\n",
    "        elif column == 'store_and_fwd_flag':\n",
    "            # Convert to bool\n",
    "            df = df.withColumn(column, \\\n",
    "                when(upper(trim(col(column))) == 'Y', 1) \\\n",
    "                .when(upper(trim(col(column))) == 'N', 0) \\\n",
    "                .otherwise(None)\n",
    "                .cast('boolean'))\n",
    "\n",
    "        # Taxi zone ID\n",
    "        elif column == 'PULocationID' or column == 'DOLocationID':\n",
    "            # Convert to int\n",
    "            df = df.withColumn(column, col(column).cast('int'))\n",
    "            # Drop rows with null and negative values and values greater than 500\n",
    "            df = df.filter(col(column).isNotNull() & (col(column) > 0) & (col(column) <= 500))\n",
    "\n",
    "        # Payment type\n",
    "        elif column == 'payment_type':\n",
    "            # Convert to int\n",
    "            df = df.withColumn(column, col(column).cast('int'))\n",
    "            # Drop negative and too big values\n",
    "            df = df.filter(col(column).isNull() | ((col(column) > 0) & (col(column) <= 1000)))\n",
    "\n",
    "        # All columns with money are treated the same way\n",
    "        elif column in money_columns:\n",
    "            # Convert to float\n",
    "            df = df.withColumn(column, col(column).cast(FloatType()))\n",
    "            # Drop negative values\n",
    "            df = df.filter(col(column).isNull() | (col(column) >= 0))\n",
    "            # Drop outliers\n",
    "            df = drop_outliers(df, column)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    df.show(5)\n",
    "    logging.info(f'Rows after cleaning: {df.count()}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d3b189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_staging_into_silver(df, silver_db):\n",
    "    logging.info(f'Start uploading data to database {silver_db}')\n",
    "\n",
    "    try:\n",
    "        # Create hash key for trips\n",
    "        df = df.withColumn(\n",
    "            'trip_hashkey',\n",
    "            sha2(concat_ws('|',\n",
    "                col('tpep_pickup_datetime'),\n",
    "                col('tpep_dropoff_datetime'),\n",
    "                col('PULocationID'),\n",
    "                col('DOLocationID'),\n",
    "                col('total_amount')\n",
    "                ),\n",
    "            256))\n",
    "\n",
    "        df.show(10)\n",
    "\n",
    "        # Write trip hub\n",
    "        df_silver = df.select(['trip_hashkey', 'record_source'])\n",
    "        df_silver.write \\\n",
    "            .format('jdbc') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{silver_db}') \\\n",
    "            .option('dbtable', 'hub_trip') \\\n",
    "            .option('user', clickhouse_user) \\\n",
    "            .option('password', clickhouse_password) \\\n",
    "            .option('batchsize', '100000') \\\n",
    "            .option('isolationLevel', 'NONE') \\\n",
    "            .option('rewriteBatchedStatements', 'true') \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        # Write trip details\n",
    "        # Exclude \"external id\" columns from dataframe\n",
    "        cols_to_exclude = ['VendorID', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type', 'load_timestamp']\n",
    "        cols = [c for c in df.columns if c not in cols_to_exclude]\n",
    "\n",
    "        df_silver = df.select(cols)\n",
    "        df_silver.write \\\n",
    "            .format('jdbc') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{silver_db}') \\\n",
    "            .option('dbtable', 'sat_trip_details') \\\n",
    "            .option('user', clickhouse_user) \\\n",
    "            .option('password', clickhouse_password) \\\n",
    "            .option('batchsize', '100000') \\\n",
    "            .option('isolationLevel', 'NONE') \\\n",
    "            .option('rewriteBatchedStatements', 'true') \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        # Write links:\n",
    "        # Trip to zone link\n",
    "        df_silver = df.select('trip_hashkey', col('PULocationID').alias('pickup_zone_id'), col('DOLocationID').alias('dropoff_zone_id'), 'record_source')\n",
    "        df_silver.write \\\n",
    "            .format('jdbc') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{silver_db}') \\\n",
    "            .option('dbtable', 'link_trip_taxi_zones') \\\n",
    "            .option('user', clickhouse_user) \\\n",
    "            .option('password', clickhouse_password) \\\n",
    "            .option('batchsize', '100000') \\\n",
    "            .option('isolationLevel', 'NONE') \\\n",
    "            .option('rewriteBatchedStatements', 'true') \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        # Trip to payment type link\n",
    "        df_silver = df.select('trip_hashkey', col('payment_type').alias('payment_type_id'), 'record_source')\n",
    "        df_silver.write \\\n",
    "            .format('jdbc') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{silver_db}') \\\n",
    "            .option('dbtable', 'link_trip_payment') \\\n",
    "            .option('user', clickhouse_user) \\\n",
    "            .option('password', clickhouse_password) \\\n",
    "            .option('batchsize', '100000') \\\n",
    "            .option('isolationLevel', 'NONE') \\\n",
    "            .option('rewriteBatchedStatements', 'true') \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        # Trip to rate link\n",
    "        df_silver = df.select('trip_hashkey', col('RatecodeID').alias('rate_id'), 'record_source')\n",
    "        df_silver.write \\\n",
    "            .format('jdbc') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{silver_db}') \\\n",
    "            .option('dbtable', 'link_trip_rate') \\\n",
    "            .option('user', clickhouse_user) \\\n",
    "            .option('password', clickhouse_password) \\\n",
    "            .option('batchsize', '100000') \\\n",
    "            .option('isolationLevel', 'NONE') \\\n",
    "            .option('rewriteBatchedStatements', 'true') \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        # Trip to vendor link\n",
    "        df_silver = df.select('trip_hashkey', col('VendorID').alias('vendor_id'), 'record_source')\n",
    "        df_silver.write \\\n",
    "            .format('jdbc') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{silver_db}') \\\n",
    "            .option('dbtable', 'link_trip_vendor') \\\n",
    "            .option('user', clickhouse_user) \\\n",
    "            .option('password', clickhouse_password) \\\n",
    "            .option('batchsize', '100000') \\\n",
    "            .option('isolationLevel', 'NONE') \\\n",
    "            .option('rewriteBatchedStatements', 'true') \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        logging.info(f'Succefully uploaded staging data to database {silver_db}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error uploading staging data to database {silver_db}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46852ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_to_silver(staging_db, silver_db):\n",
    "    \"\"\"\n",
    "    Process data from staging table and insert into silver layer\n",
    "    \"\"\"\n",
    "    # Process staging data using Spark\n",
    "    try:\n",
    "        spark.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('Staging2Silver') \\\n",
    "        .master(spark_master) \\\n",
    "        .config('spark.driver.host', 'host.docker.internal') \\\n",
    "        .config('spark.driver.bindAddress', '0.0.0.0') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .format('jdbc') \\\n",
    "            .option('url', f'jdbc:clickhouse://{clickhouse_host}:{clickhouse_port}/{clickhouse_staging_db_name}') \\\n",
    "            .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n",
    "            .option('dbtable', 'staging_data') \\\n",
    "            .option('user', f'{clickhouse_user}') \\\n",
    "            .option('password', f'{clickhouse_password}') \\\n",
    "            .load()\n",
    "        logging.info(f'Connection to database {staging_db} successful. Processing data for silver layer.')\n",
    "        df.show(5)\n",
    "\n",
    "        # Clean data\n",
    "        df = clean_staging_data(df)\n",
    "\n",
    "        # Insert data into silver layer\n",
    "        insert_staging_into_silver(df, silver_db)\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error connecting to database {staging_db} from Spark cluster: {e}')\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            spark.stop()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ba0993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark master доступен!\n"
     ]
    }
   ],
   "source": [
    "def is_spark_master_available(host='localhost', port=7077, timeout=3):\n",
    "    try:\n",
    "        with socket.create_connection((host, port), timeout=timeout):\n",
    "            return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "if is_spark_master_available():\n",
    "    print('Spark master доступен!')\n",
    "else:\n",
    "    print('Не удалось подключиться к Spark master.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffa7979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_to_gold(file_name):\n",
    "    \"\"\"\n",
    "    Create datamarts from silver layer\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\t\\t', file_name, 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Connection to database nyc_taxi_staging successful. Processing data for silver layer.\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|     load_timestamp|       record_source|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+\n",
      "|       1| 2017-01-01 03:32:05|  2017-01-01 03:37:48|              1|          1.2|         1|                 N|         140|         236|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       1| 2017-01-01 03:43:25|  2017-01-01 03:47:42|              2|          0.7|         1|                 N|         237|         140|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       1| 2017-01-01 03:49:10|  2017-01-01 03:53:53|              2|          0.8|         1|                 N|         140|         237|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       1| 2017-01-01 03:36:42|  2017-01-01 03:41:09|              1|          1.1|         1|                 N|          41|          42|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       1| 2017-01-01 03:07:41|  2017-01-01 03:18:16|              1|          3.0|         1|                 N|          48|         263|           2|       11.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.3|2025-11-13 21:20:58|http://def-minio:...|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "INFO:root:Rows before cleaning: 100000\n",
      "INFO:root:Deleted outliers for column trip_distance\n",
      "INFO:root:Deleted outliers for column fare_amount\n",
      "INFO:root:Deleted outliers for column extra\n",
      "INFO:root:Deleted outliers for column mta_tax\n",
      "INFO:root:Deleted outliers for column tip_amount\n",
      "INFO:root:Deleted outliers for column tolls_amount\n",
      "INFO:root:Deleted outliers for column improvement_surcharge\n",
      "INFO:root:Deleted outliers for column total_amount\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|     load_timestamp|       record_source|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+\n",
      "|       1| 2017-01-01 03:03:24|  2017-01-01 03:06:06|              1|          0.4|         1|             false|         224|           4|           2|        4.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.3|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       2| 2017-01-01 03:05:16|  2017-01-01 03:07:53|              2|         0.36|         1|             false|          79|           4|           1|        3.5|  0.5|    0.5|      0.96|         0.0|                  0.3|        5.76|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       1| 2017-01-01 03:04:03|  2017-01-01 03:11:07|              2|          1.0|         1|             false|         113|           4|           1|        6.5|  0.5|    0.5|       2.0|         0.0|                  0.3|         9.8|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       1| 2017-01-01 03:05:39|  2017-01-01 03:11:18|              1|          1.1|         1|             false|          79|           4|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|2025-11-13 21:20:58|http://def-minio:...|\n",
      "|       2| 2017-01-01 03:07:29|  2017-01-01 03:11:21|              5|         1.17|         1|             false|         148|           4|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|2025-11-13 21:20:58|http://def-minio:...|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "INFO:root:Rows after cleaning: 88431\n",
      "INFO:root:Start uploading data to database nyc_taxi_silver\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|     load_timestamp|       record_source|        trip_hashkey|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+--------------------+\n",
      "|       1| 2017-01-01 03:03:24|  2017-01-01 03:06:06|              1|          0.4|         1|             false|         224|           4|           2|        4.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.3|2025-11-13 21:20:58|http://def-minio:...|e256cf6441d9ba43f...|\n",
      "|       2| 2017-01-01 03:05:16|  2017-01-01 03:07:53|              2|         0.36|         1|             false|          79|           4|           1|        3.5|  0.5|    0.5|      0.96|         0.0|                  0.3|        5.76|2025-11-13 21:20:58|http://def-minio:...|9fe11229245ec4fdb...|\n",
      "|       1| 2017-01-01 03:04:03|  2017-01-01 03:11:07|              2|          1.0|         1|             false|         113|           4|           1|        6.5|  0.5|    0.5|       2.0|         0.0|                  0.3|         9.8|2025-11-13 21:20:58|http://def-minio:...|0586695b39d46fc4d...|\n",
      "|       1| 2017-01-01 03:05:39|  2017-01-01 03:11:18|              1|          1.1|         1|             false|          79|           4|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|2025-11-13 21:20:58|http://def-minio:...|7bf4e14625aec5250...|\n",
      "|       2| 2017-01-01 03:07:29|  2017-01-01 03:11:21|              5|         1.17|         1|             false|         148|           4|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|2025-11-13 21:20:58|http://def-minio:...|bee3fe4e54ee1c8b9...|\n",
      "|       2| 2017-01-01 03:07:40|  2017-01-01 03:11:30|              6|         0.97|         1|             false|         148|           4|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|2025-11-13 21:20:58|http://def-minio:...|a3c854cc0c56db473...|\n",
      "|       2| 2017-01-01 03:13:12|  2017-01-01 03:15:23|              5|         0.54|         1|             false|           4|           4|           2|        4.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.3|2025-11-13 21:20:58|http://def-minio:...|6c991737c3c12dae0...|\n",
      "|       2| 2017-01-01 03:08:22|  2017-01-01 03:17:22|              1|         0.73|         1|             false|         232|           4|           1|        7.0|  0.5|    0.5|       2.0|         0.0|                  0.3|        10.3|2025-11-13 21:20:58|http://def-minio:...|5022baf75b993d33e...|\n",
      "|       2| 2017-01-01 03:10:07|  2017-01-01 03:19:18|              1|         1.02|         1|             false|         148|           4|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|2025-11-13 21:20:58|http://def-minio:...|ca2c166922ade39bd...|\n",
      "|       1| 2017-01-01 03:13:23|  2017-01-01 03:21:20|              1|          1.2|         1|             false|         211|           4|           1|        7.5|  0.5|    0.5|      1.75|         0.0|                  0.3|       10.55|2025-11-13 21:20:58|http://def-minio:...|fa766debc05450125...|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "INFO:root:Succefully uploaded staging data to database nyc_taxi_silver\n"
     ]
    }
   ],
   "source": [
    "# For each downloaded and unprocessed file with raw data\n",
    "# - import to staging\n",
    "# - process to silver\n",
    "# - process to gold\n",
    "unprocessed_files_list = get_unprocessed_files_list(clickhouse_client, clickhouse_staging_db_name)\n",
    "\n",
    "for file in unprocessed_files_list:\n",
    "    #bronze_to_staging(clickhouse_client, clickhouse_staging_db_name, file, bronze_storage)\n",
    "    #staging_to_silver(clickhouse_staging_db_name, clickhouse_silver_db_name)\n",
    "    #silver_to_gold(file)\n",
    "    #set_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39999ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8221d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b47e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
